{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "whole-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fatty-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Модель с последовательным проходом по слоям\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Слои: полносвязный, Dropout, входной слой, вытягивающий в одномерный вектор 784х1\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# Различные виды вариантов расчета градиентного спуска\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Nadam\n",
    "\n",
    "# Чтобы сделать выходы one-hot \n",
    "from tensorflow.keras import utils\n",
    "\n",
    "# Инициализация параметров слоев\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "# Критерии качества\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# Не углублялся, но что-то вроде критерия останова\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "northern-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liquid-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация к границам [0;1]\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "funded-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "Y_train = utils.to_categorical(y_train, 10)\n",
    "Y_test = utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Общее описание структуры нейросети\n",
    "# Я использовал только 1 скрытый слой, теоретически можно через тюнер в цикле перебирать кол-во слоев\n",
    "# также использовал только categorical_crossentropy в качестве критерия качества, можно перебирать и другие\n",
    "def call_existing_code(units, activation, dropout, lr, do_rate, optimizer, nesterov, centered, mean, stddev, value):\n",
    "    model = Sequential()\n",
    "    \n",
    "    init_weights = initializers.TruncatedNormal(mean=mean, stddev=stddev) # инициализация весов\n",
    "    init_biases = initializers.Constant(value=value) # инициализация свободных членов\n",
    "    \n",
    "    model.add(Flatten()) # входной слой, \"вытягивает\" 28x28 в 784x1\n",
    "    \n",
    "    # Полносвязный слой с настройками, описанными в функции ниже\n",
    "    model.add(Dense(units=units, activation=activation, kernel_initializer=init_weights, bias_initializer=init_biases))\n",
    "    \n",
    "    # Использовать ли dropout\n",
    "    if dropout:\n",
    "        model.add(Dropout(rate=do_rate))\n",
    "        \n",
    "    # Выходной слой на 10 выходов с softmax\n",
    "    model.add(Dense(10, activation=\"softmax\", kernel_initializer=init_weights, bias_initializer=init_biases))\n",
    "    \n",
    "    # Перебор различных оптимизаторов\n",
    "    if optimizer == \"Nadam\":\n",
    "        model.compile(\n",
    "            optimizer=Nadam(learning_rate=lr),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "    elif optimizer == \"SGD\":\n",
    "        model.compile(\n",
    "            optimizer=SGD(learning_rate=lr, nesterov=nesterov),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        model.compile(\n",
    "            optimizer=RMSprop(learning_rate=lr, centered=centered),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Функция задания тюнером параметров для перебора\n",
    "def model_builder(hp):\n",
    "    \n",
    "    # Кол-во нейронов, используется только в скрытом слое. Инициализируется как целое число, ранжирование задается с шагом\n",
    "    units = hp.Int(\"units\", min_value=128, max_value=512, step=128)\n",
    "    \n",
    "    # Выбор активационной функции скрытого слоя. Инициализируется как Enum (просто сравнение)\n",
    "    activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "    \n",
    "    # Логическая переменная, переберет всего лишь True и False, от которой будет зависеть, применять ли dropout\n",
    "    dropout = hp.Boolean(\"dropout\")\n",
    "    \n",
    "    # Перебор оптимизаторов\n",
    "    optimizer = hp.Choice(\"optimizer\", [\"Nadam\", \"SGD\", \"RMSprop\"])\n",
    "    \n",
    "    # Скорость обучения, используется во всех оптимизаторах. Инициализируется как число с плавающей запятой, перебор с логарифмическим шагом\n",
    "    lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    \n",
    "    # Процент неиспользуемых нейронов при dropout. Инициализируется как число с плавающей запятой, перебор с шагом\n",
    "    do_rate = hp.Float(\"rate\", min_value=2.5e-1, max_value=5e-1, step=2.5e-1)\n",
    "    \n",
    "    # Использовать ли поправку Нестерова при оптимизаторе SGD\n",
    "    nesterov = hp.Boolean(\"nesterov\")\n",
    "    \n",
    "    # Нормализовать ли градиенты (???) при оптимизаторе RMSprop\n",
    "    centered = hp.Boolean(\"centered\")\n",
    "    \n",
    "    # Среднее значение весов при инициализации\n",
    "    mean = hp.Float(\"mean\", min_value=0.0, max_value=1e-1, step=2.5e-2)\n",
    "    \n",
    "    # Среднеквадратическое отклонение весов при инициализации\n",
    "    stddev = hp.Float(\"stddev\", min_value=2.5e-1, max_value=1, step=2.5e-1)\n",
    "    \n",
    "    # Значение свободных членов при инициализации (вроде можно инициализировать тоже через распределение с mean и stddev)\n",
    "    value = hp.Float(\"value\", min_value=0, max_value=1e-2, step=5e-3)\n",
    "    # Вызывает функцию call_existing_code, подставляя в нее параметры\n",
    "    model = call_existing_code(\n",
    "        units=units, activation=activation, dropout=dropout, lr=lr, do_rate=do_rate, optimizer=optimizer, \n",
    "        nesterov=nesterov, centered=centered, mean=mean, stddev=stddev, value=value\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_builder(kt.HyperParameters()) # Проверяет, скомпилится ли модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формирование тюнера, все стадии обучения будут сохраняться в папку directory\n",
    "tuner = kt.Hyperband(model_builder, # Настроенная сетка параметров\n",
    "                     objective='val_accuracy', # По каким данным смотреть улучшение модели\n",
    "                     max_epochs=20,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt',\n",
    "                     overwrite=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Критерий останова по валидационным данным, если точность на них ухудшается или не двигается patience раз\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОБУЧЕНИЕ\n",
    "tuner.search(X_train, Y_train, epochs=50, validation_split=0.2, callbacks=[TensorBoard(\"/my_dir/intro_to_kt/my_logs\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir /my_dir/intro_to_kt/my_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение наилучших параметров\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Лучшие параметры:\n",
    "Кол-во нейронов скрытого слоя: {best_hps.get('units')}\n",
    "Активационная функция скрытого слоя: {best_hps.get('activation')}\n",
    "Использовать ли Dropout: {best_hps.get('dropout')}\n",
    "Процент неиспользуемых нейронов при Dropout: {best_hps.get('rate')}\n",
    "Оптимизатор: {best_hps.get('optimizer')}\n",
    "Скорость обучения: {best_hps.get('lr')}\n",
    "Использовать ли поправку Нестерова, если оптимизатор SGD: {best_hps.get('nesterov')}\n",
    "Нормализовать ли градиенты, если оптимизатор RMSprop: {best_hps.get('centered')}\n",
    "Среднее значение весов при инициализации: {best_hps.get('mean')}\n",
    "Среднеквадратическое отклонение весов при инициализации: {best_hps.get('stddev')}\n",
    "Значение свободных членов при инициализации: {best_hps.get('value')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нахождение оптимального числа эпох\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, Y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
